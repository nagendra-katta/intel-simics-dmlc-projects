<!doctype html>
<head>
<meta charset="utf-8">
<title>4.1 Simulation Performance</title>
<link rel="stylesheet" href="../simics.css">
<script src="../page-script.js"></script>
</head>
<div class="chain">
<a href="performance.html">4 Performance</a>
<a href="scaling-simics.html">4.2 Scaling Simics</a>
</div>
<div class="path">
<a href="index.html">Simics User's Guide</a>
&nbsp;/&nbsp;
<a href="performance.html">4 Performance</a>
&nbsp;/&nbsp;</div><h1 id="simulation-performance"><a href="#simulation-performance">4.1 Simulation Performance</a></h1>
<p>This chapter covers various topics related to Simics performance and what can be done to measure and improve it. It discusses the general performance features provided by Simics. For ways to scale the simulation even further see chapter <a class="reference" href="scaling-simics.html">4.2</a>.</p>
<p>Simics is a fast simulator utilizing various techniques such as run-time code generation to optimize performance. In some cases Simics can execute code faster than the target system being simulated, while it can also be considerably slower in other cases.</p>
<p>There are four major execution modes Simics uses to execute target instructions: hypersimulation, VMP, JIT and interpreted mode.</p>
<p>Hypersimulation means that Simics detects repetitive work performed by the target code and performs the effects of the code without actually having to run the code. In the most simple case this is a simple idle loop, but it can also be applied to more complex examples such as spin-locks and device polling. This is the fastest execution mode.</p>
<p>VMP, which is a part of Simics's x86 models, utilizes the virtualization capabilities of modern processors to run target instructions directly. This typically results in high simulation performance, but the host and target needs have the same instruction set, and you have to do special set up to enable it. VMP is currently only supported on x86 hosts.</p>
<p>JIT mode uses run-time code generation to translate blocks of the target instructions into blocks of host instructions. JIT mode is when Simics runs such translated blocks. This mode is supported by most target processor models in Simics.</p>
<p>Interpreted mode interprets the target instructions one by one. This mode is the slowest, but it is always available.</p>
<p>There are basically two ways to measure Simics performance:</p>
<ul>
<li>How fast the instructions are being simulated, typically measured in million target instructions per host second (MIPS).</li>
<li>How fast the virtual times elapses.</li>
</ul>
<p>In most cases the user is mostly interested in the first. Simics should execute instructions as fast as possible to finish the workload in shortest possible time. However, since Simics is a full system simulator, it is also important that the virtual time on the simulated machine advances quickly. That is important in cases where a program or operating system is waiting on a timer to expire or an interrupt from a device in order to proceed with the workload.</p>
<p>If we divide the wall-clock time on the host that Simics executes on, with the elapsed virtual time on the target machine, we get a slowdown number.</p>
<p>slowdown = Time<sub>host</sub>/Time<sub>virtual</sub></p>
<p>A slowdown number of 2.3 means that Simics performance is 2.3 times slower than the system it simulates. A slowdown value of less than 1.0 means that Simics manages to execute the corresponding code faster than the system it simulates. The slowdown depends on various factors:</p>
<ul>
<li>The performance of the host which Simics runs on.</li>
<li>The application which runs in Simics.</li>
<li>The frequency of the target being simulated.</li>
<li>The simulator time model.</li>
</ul>
<p>The default time model in Simics is that each target instruction takes one target cycle to execute. That is the default, <em>Instructions Per Cycle</em> (IPC) is 1.0. This is a simplification (but in many cases an adequate approximation) compared to the actual time it takes on the real hardware to execute instructions. It is possible to change the IPC number using the <code>&lt;cpu&gt;.set-step-rate</code> command. For example:</p>
<pre><code class="language-simics">simics&gt; <strong>board.mb.cpu0.core[0][0].set-step-rate ipc = 1.5</strong>
Setting step rate to 3/2 steps/cycle
simics&gt; <strong>board.mb.cpu0.core[0][0].set-step-rate ipc = 0.5</strong>
Setting step rate to 1/2 steps/cycle
</code></pre>
<p>In the first example, IPC of 1.5 means that Simics needs to execute 3 instructions for 2 cycles to elapse. In the second example, for each instruction executed two cycles elapse. Thus, with a lower IPC value, virtual time will progress faster and simulation slowdown will decrease.</p>
<p>Note that there is nothing wrong in changing the default IPC when it comes to the accuracy of the simulation. In many cases, the IPC observed for a given benchmark is much lower than the 1.0 that Simics assumes, and matching it will both make the simulation closer to the real hardware and improve the simulation speed, at least in virtual time. Simulations that profits most from this change are simulations involving devices and long memory latencies.</p>
<h2 id="measuring-performance"><a href="#measuring-performance">4.1.1 Measuring Performance</a></h2>
<p>The <code>system-perfmeter</code> extension can be used to understand the performance you get out of Simics. The system-perfmeter is sample based, which means that you can see the performance during the workload execution, and how it varies, not only the end result when a workload is finished.</p>
<p>The easiest way to try it out is simply to issue the <code>system-perfmeter</code> command without any additional arguments:</p>
<pre><code class="language-simics">simics&gt; <strong>system-perfmeter</strong>
</code></pre>
<p>This will cause a sample to be taken every 1.0 virtual seconds. For each sample the system-perfmeter extracts various counters from Simics and displays the delta since last time. The output can look like this:</p>
<pre><code class="language-simics">simics&gt; <strong>c</strong>
SystemPerf: Total-vt Total-rt Sample-vt Sample-rt Slowdown  CPU Idle
SystemPerf: -------- -------- --------- --------- -------- ---- ----
SystemPerf:     1.0s     6.0s     1.00s     5.99s     5.99  97%   0%
SystemPerf:     2.0s     6.7s     1.00s     0.69s     0.69  97%   0%
SystemPerf:     3.0s     8.0s     1.00s     1.34s     1.34  92%   0%
SystemPerf:     4.0s     8.4s     1.00s     0.42s     0.42 100%   0%
SystemPerf:     5.0s     9.2s     1.00s     0.78s     0.78  98%  14%
SystemPerf:     6.0s    10.5s     1.00s     1.31s     1.31  96%  55%
SystemPerf:     7.0s    10.7s     1.00s     0.12s     0.12  92%  93%
SystemPerf:     8.0s    10.7s     1.00s     0.00s     0.00 100% 100%
</code></pre>
<p>Here we can see the execution for the first 8 virtual seconds and the corresponding performance measured in each second sample. To simulate these 8 virtual seconds, it took Simics 10.7 host seconds, thus the average slowdown is 1.34.</p>
<p>The <code>CPU</code> column shows how much of the host CPU that Simics has used, allowing you to notice if there is another process consuming the host CPU resources. Another reason for CPU utilization to be low can be that Simics itself is running in real-time mode where Simics sleeps so that virtual time does not race ahead of host time.</p>
<p>When an 'idle' condition has been detected (see chapter <a class="reference" href="#idle-loops-and-performance">4.1.4.1</a>), the total idleness of the system is reported in the <code>Idle</code> column. If the simulated system consists of multiple processors and you wish to see how much each processor is idling you can use the <em>-cpu-idle</em> switch to <code>system-perfmeter</code>. Note that idling is defined by the simulator, not by the target architecture (see chapter <a class="reference" href="#idle-loops-and-performance">4.1.4.1</a>). With the <em>-cpu-exec-mode</em>, information is also gathered and printed on how simulation steps are executed in the CPU model. The fastest mode to be executing in is idle, followed by VMP, JIT, and interpreter.</p>
<p>The <code>system-perfmeter</code> can also be used to get an understanding of which processor that takes the longest time to simulate. The <em>-cpu-host-ticks</em> switch adds extra columns per CPU for this. For example:</p>
<pre><code>SystemPerf: Total-vt Total-rt Sample-vt Sample-rt Slowdown  CPU Idle [   0   1   2   3 ]
SystemPerf: -------- -------- --------- --------- -------- ---- ----   --- --- --- ---
SystemPerf:     1.0s    15.6s     1.00s    15.63s     15.6  99%  74% [  71  11  11   8 ]
SystemPerf:     2.0s    32.6s     1.00s    16.97s     17.0  98%  72% [  70  11  10   9 ]
</code></pre>
<p>Here we have a 4 CPU system which is idle roughly 70% and the last columns show that CPU0 takes 70% of the time to simulate, while the other three about 10% each. Hence CPU0 is working while the other CPUs are idling.</p>
<p>The <em>-module-profile</em> flag enables live profiling of the main Simics thread. The profiling is sample based, and any sample hitting in code produced dynamically by a JIT engine will be reported as <code>"classname JIT"</code>. The <em>-module-profile</em> data is not printed in the standard line print mode, so you must use either <em>-top</em> or <em>-summary</em> to get profiling information.</p>
<figure id="perfmeter-fig">
<p><img alt="" src="perfmeter_description.png"> </p><figcaption>Figure 31. Annotated output from <code>system-perfmeter -top -module-profile -cpu-exec-mode</code> </figcaption><p></p>
</figure>
<p>Figure <a class="reference" href="#perfmeter-fig">31</a> explains the output of <code>system-perfmeter</code>. There are many other options to the system-perfmeter command, please read the associated help text for details.</p>
<pre><code class="language-simics">simics&gt; <strong>help system-perfmeter</strong>
</code></pre>
<h2 id="multithreaded-simulation-profiling-and-tuning"><a href="#multithreaded-simulation-profiling-and-tuning">4.1.2 Multithreaded Simulation Profiling and Tuning</a></h2>
<p>With Simics Accelerator, simulation performance can be increased by utilizing host system parallelism. Simics exploits available target system parallelism and spreads the load over available host cores, in order to increase performance.</p>
<p>Simics Accelerator has two different mechanisms that can operate alone or work together to improve performance. The first is Simics® <em>Multimachine Accelerator</em> which is based upon the cell concept. The other mechanism is <em>Multicore Accelerator</em> which can parallelize simulation even within cells.</p>
<p>A cell ideally contains tightly coupled parts of the target system (typically one or more CPUs and associated devices). Different cells can be simulated in parallel with <em>Multimachine Accelerator</em> (which is default on), but unless <em>Multicore Accelerator</em> is enabled (default off), a single cell will be simulated in a single-threaded fashion (see chapter <a class="reference" href="scaling-simics.html">4.2</a> for more details).</p>
<h3 id="simics-multimachine-accelerator"><a href="#simics-multimachine-accelerator">4.1.2.1 Simics® Multimachine Accelerator</a></h3>
<p>The Multithreaded Simulation Profiler tool (<strong>mtprof</strong>) can be used to analyze some performance aspects of multithreaded simulation. Mtprof is primarily useful to</p>
<ul>
<li>detect parallelism bottlenecks</li>
<li>predict how fast the simulation would run on a more parallel host machine</li>
<li>provide some insights into the performance impact of various latency setting.</li>
</ul>
<p>The Multithreaded Simulation Profiler helps in understanding threading behavior at the Multimachine Accelerator level. There is no explicit tool support for optimizing specific Multicore Accelerator issues.</p>
<p>The Multithreaded Simulation Profiler is started with the following command:</p>
<pre><code class="language-simics">simics&gt; <strong>enable-mtprof</strong>
</code></pre>
<p>When mtprof is enabled, Simics keeps track of how much CPU time is spent simulating each cell in the system. Once Simics has been running for a while, it is possible to ask mtprof for an overview:</p>
<pre><code class="language-simics">simics&gt; <strong>mtprof.cellstat</strong>
============================================================
  cellname                    rt   %elapsed  %cputime
------------------------------------------------------------
  ebony3.cell3             77.0s     93.6%    80.0% 
  ebony1.cell1              8.9s     10.9%     9.3% 
  ebony2.cell2              8.2s     10.0%     8.5% 
  ebony0.cell0              2.1s      2.5%     2.1% 
------------------------------------------------------------
  elapsed_realtime         82.2s    100.0%    85.5%
============================================================
</code></pre>
<p>From the above output, we can conclude that cell3 is a limiting factor: the other cells frequently have to wait upon this cell in order to keep the virtual time in sync. Investigating why cell3 is so expensive to simulate is the next natural step: there might be an expensive poll loop or the idle optimization might not function properly, for instance. Other potential ways to address the load imbalance include</p>
<ul>
<li>enabling <em>Multicore Accelerator</em> for the cell if applicable</li>
<li>splitting cell3 into multiple cells (if it contains multiple CPUs and Multicore Accelerator is not applicable)</li>
<li>changing the step rate of CPUs belonging to cell3.</li>
</ul>
<p>The mtprof tool can also be used to estimate how fast the simulation would run on a machine with enough host cores to allow Simics to assign a dedicated host core to each cell:</p>
<pre><code class="language-simics">simics&gt; <strong>mtprof.modelstat</strong>
============================================================
  latency      rt_model  realtime/rt_model
------------------------------------------------------------
    10 ms        81.9s       100.4%
    40 ms        81.2s       101.2%
   160 ms        80.3s       102.4%
   640 ms        78.6s       104.6%
============================================================
</code></pre>
<p>The latency column corresponds to various values for <code>set-min-latency</code>. In this case, the simulation was executed with a 10 ms latency, which means that the model predicts that the simulation would take 81.9 s to run (which is pretty close to the measured value of 82.2 s above).</p>
<p>It is important to note that the performance model does not take modified target behavior, due to different latency settings, into account (which can be a huge factor if the cells interact). With substantial inter-cell interaction, only the row corresponding to the current latency setting should be trusted.</p>
<p>Below is an example from a more evenly loaded system run with a min-latency of 1 ms:</p>
<pre><code class="language-simics">simics&gt; <strong>mtprof.modelstat</strong>
============================================================
  latency      rt_model  realtime/rt_model
------------------------------------------------------------
     1 ms        35.3s       107.4%
     4 ms        29.5s       128.8%
    16 ms        25.9s       146.3%
    64 ms        24.2s       156.5%
============================================================
</code></pre>
<p>In this case, we see that increasing the latency setting to about 16 ms would improve simulation performance substantially (once again, without taking changed target behavior into account).</p>
<p>Quite often, the target behavior is not static but varies with simulated time. In that case, it is often useful to export the collected data and plot it using an external tool:</p>
<pre><code class="language-simics">simics&gt; <strong>mtprof.save-data output.txt</strong>
</code></pre>
<p>The exported data is essentially the information provided by <code>mtprof.cellstat</code> and/or <code>mtprof.modelstat</code>, but expressed as a function of virtual time (exported in a plot friendly way). The <code>mtprof.save-data</code> command takes multiple flags which can be used to customize the output. One useful flag is <em>-oplot</em>:</p>
<pre><code class="language-simics">simics&gt; <strong>mtprof.save-data mtprof-plot.m -oplot</strong>
</code></pre>
<p>which outputs the data in the form of an Octave file together with commands which plots the data.</p>
<p>The mtprof tool currently does not support Multicore Accelerator.</p>
<h3 id="multicore-accelerator"><a href="#multicore-accelerator">4.1.2.2 Multicore Accelerator</a></h3>
<p>Whether to use Multicore Accelerator or not depends heavily on the workload of the system that is being modeled. Multicore Accelerator performs best for systems with CPU intensive tasks, with little communication, and a low I/O rate. The <code>print-device-access-stats</code> and <code>system-perfmeter</code> commands can be use measure the frequency of I/O operations in the system. (I/O operations are expensive in Multicore Accelerator mode.)</p>
<p>Note that some systems perform well even if the number of target instructions per I/O operation is as low as 300 while others perform bad even though this number is considerably higher. Different target systems behave differently, and in the end, one have to benchmark each system by itself to get highest possible performance. As a guideline, if a target system executes less than 10,000 instructions per I/O operation on average, one might have to consider switching off Multicore Accelerator, or tune the system software in order to achieve good performance.</p>
<p>The <code>enable-multicore-accelerator</code> command provides some tuning and settings for Multicore Accelerator. For more information, see the documentation for the <code>enable-multicore-accelerator</code> command.</p>
<p>It is probably best practice to try to optimize the cell partitioning as described in section <a class="reference" href="#simics-multimachine-accelerator">4.1.2.1</a> to find the most resource demanding cells first, and then switching on Multicore Accelerator for those cells (if available host cores still exists). Currently, it is not possible to restrict a number of host cores for particular target CPUs. Simics will instead automatically balance the system as efficient as possible.</p>
<h2 id="platform-effects"><a href="#platform-effects">4.1.3 Platform Effects</a></h2>
<p>Starting with the most important, consider these factors when choosing a platform to run Simics on:</p>
<div class="dl">
<ul>
<li>
<p><span class="term" id="dt:cpu-speed"><a href="#dt:cpu-speed">CPU Speed</a></span><br>
Simulation is a very compute intensive application. As a general rule, a machine with higher compute performance will outperform a lower compute performance machine unless the simulation is starved for memory. The SPECint2006 benchmark suite (<code>www.spec.org</code>) is a good indicator of compute performance and can be used to compare systems with different processors. Both processor architecture and clock frequency affects the performance. Within an architecture performance scales almost linearly with clock frequency.</p>
<p>Other CPU features such as the size of the caches does affect performance, but less so than the architecture and the clock frequency.</p>
</li>
<li>
<p><span class="term" id="dt:memory-size"><a href="#dt:memory-size">Memory Size</a></span><br>
The simulator can operate with less memory than the simulated memory size. It does that by allocating pages only when used and swapping out pages to disk when running low on memory. The memory size needs to be large enough so that the paging of memory to disk does not hurt performance too much.</p>
</li>
<li>
<p><span class="term" id="dt:number-of-cores"><a href="#dt:number-of-cores">Number of Cores</a></span><br>
Simics Accelerator will utilize multiple host threads to simulate in parallel. If all simulated processors require equal amounts of time to simulate, then the simulator can theoretically utilize as many host cores as there are simulated processors. In practice, the workload is always skewed, meaning that fewer host cores can be effectively utilized. It is more efficient to use multiple cores or processors on a single machine compared to distributing the simulation across multiple host machines.</p>
</li>
<li>
<p><span class="term" id="dt:file-system"><a href="#dt:file-system">File System</a></span><br>
Running from local disk is faster than running over a network file system. When running low on memory, make sure that the swap directory used by the memory limit feature is located on local disk. Also, solid state disks (SSDs) are highly preferred over traditional rotating disks.</p>
</li>
</ul>
</div>
<h2 id="workload-characteristics"><a href="#workload-characteristics">4.1.4 Workload Characteristics</a></h2>
<p>The performance of Simics sometimes depends on the kind of software that runs inside the target machine being simulated. Thus, the slowdown can vary a lot depending on what the target software is currently executing. Here are some general tips for understanding what decreases simulation speed.</p>
<div class="dl">
<ul>
<li><span class="term" id="dt:floating-point-intense-workloads"><a href="#dt:floating-point-intense-workloads">Floating-point intense workloads</a></span><br>
If the target software runs floating-point arithmetic instructions frequently, the performance is likely lower compared to running integer based workloads. The same is usually true for workloads with frequent use of SIMD-style media instructions.</li>
<li><span class="term" id="dt:supervisor-code"><a href="#dt:supervisor-code">Supervisor code</a></span><br>
If a workload runs much of the code in supervisor mode such as frequently causing exceptions, this type of code normally runs slower than regular user-level code.</li>
<li><span class="term" id="dt:i-o-workloads"><a href="#dt:i-o-workloads">I/O workloads</a></span><br>
If the code does frequent accesses to devices, compared to accessing RAM, Simics needs to do more work to simulate this, which slows down simulation. Note that this type of workload is an excellent candidate for a lower IPC setting.</li>
<li><span class="term" id="dt:memory-usage"><a href="#dt:memory-usage">Memory usage</a></span><br>
If the target software is using more memory than available on the host. This is typically a problem when the simulated machine has equal or more memory than the host it executes on <strong>and</strong> the software also uses it. This will cause Simics to swap out pages on disk, which decreases performance.</li>
<li><span class="term" id="dt:event-usage"><a href="#dt:event-usage">Event usage</a></span><br>
Simics modules which frequently post events with short time quanta. For example, if a device posts an event every 10th cycle, to keep a counter register updated, this will severely affect performance.</li>
</ul>
</div>
<p>Typically, target code which runs with low performance on real hardware due to bad cache behavior, bad memory locality etc. will also cause Simics to run with poor performance.</p>
<h3 id="idle-loops-and-performance"><a href="#idle-loops-and-performance">4.1.4.1 Idle Loops and Performance</a></h3>
<p>When an operating system does not have any processes to schedule, it typically runs some tight loop waiting for an interrupt to occur. This is referred to as the <em>idle loop</em>. The way the idle loop is implemented varies between operating systems and the capabilities in the underlying hardware.</p>
<p>For example, the most simple idle loop would be a "branch to itself" instruction. When the processor reaches this instruction, nothing but an interrupt will cause the execution to proceed somewhere else. Another example is when the operating system uses some kind of power-down mode on the processor, causing the processor to stop executing any more instructions (and consequently consume less power). Some processors also have dedicated instructions causing the processor to stop until something interesting happens, such as the x86 <code>HLT</code> instruction. Processor idling in Simics is defined by what the simulator can detect and usually includes architectural states such as halt or power-down, but can also be loops normally executed by the processor.</p>
<p>A fast simulation of the idle loop, is very important in some cases. For example, when simulating multiple processors, we want to use as much of the host CPU cycles as possible for simulating the processor that actually performs useful tasks. Rather than wasting cycles on the idle loop.</p>
<p>Even when simulating a single CPU, fast idle loops can be important, since all of the active processes might be stalling on disk or some other peripheral. Execution of the processes will not continue unless, for example, a disk issues an interrupt.</p>
<p>Simics processor models can sometimes detect idle conditions. When the processor model detects a branch to itself, there is no point in simulating the instruction, if it branches to itself repeatedly.</p>
<p>Instead, Simics is capable of fast-forwarding time until an event that can generate an interrupt is about to be executed. Hence, this model is equivalent to running the branch millions of times, but is much faster.</p>
<p>In some cases, the idle loop in the operating system is more than a single instruction, e.g., it might be a loop checking a variable in memory for the next process to schedule. These more difficult cases can be handled with hypersimulation (see chapter <a class="reference" href="#hypersimulation">4.1.5</a>).</p>
<h2 id="hypersimulation"><a href="#hypersimulation">4.1.5 Hypersimulation</a></h2>
<p>The term <em>hypersimulation</em> refers to a simulator feature which can detect, analyze and understand, frequently executed target instructions and fast-forward the simulation of these, thus providing the corresponding results more rapidly.</p>
<p>Being able to detect the idle loop (see chapter <a class="reference" href="#idle-loops-and-performance">4.1.4.1</a>) is one example of when this technique is applicable. A much more extreme hypersimulation task would be to understand a complete program and simply provide the corresponding result without actually starting the program. Naturally, this is hardly ever applicable, and impossible in general. Busy-wait loops and spin-locks are more realistic examples of cases where it is easy to optimize away the execution with hypersimulation.</p>
<p>Hypersimulation can be achieved in several ways:</p>
<ul>
<li>
<p><strong>CPU handled instruction hypersimulation</strong>: The processor model can detect certain instructions which will either stop the execution or jump to itself. This behavior is always enabled, thus <code>disable-hypersim</code> does not change the behavior. Only an exception/interrupt will stop this execution.</p>
</li>
<li>
<p><strong>Automatic hypersimulation</strong>: Automatic detection of certain small loops that only wait for a future event to occur. This feature is only available for some processor models. The <code>-no-auto</code> switch for the <code>enable-hypersim</code> command disables automatic hypersimulation.</p>
</li>
<li>
<p><strong>Hypersim-pattern-matcher</strong>: User-written specifications, hypersim patterns, are used by the simulator to detect waiting loops that match. The hypersim pattern describes the binary layout of the instructions in the loop and the conditions for leaving it. The hypersim-pattern-matcher module contains the framework for writing hypersim patterns.</p>
</li>
</ul>
<p>The following instructions are handled with <em>CPU handled instruction hypersimulation</em>:</p>
<table><thead><tr><th><strong>Target</strong></th><th><strong>Instruction</strong></th><th><strong>Comment</strong></th></tr></thead><tbody>
<tr><td>ARM</td><td><code>mcr</code></td><td>Enabling "Wait for Interrupt"</td></tr>
<tr><td>m68k</td><td><code>stop</code></td><td></td></tr>
<tr><td>MIPS</td><td><code>wait</code></td><td></td></tr>
<tr><td>PowerPC</td><td><code>mtmsr</code></td><td>Setting <code>MSR[POW]</code>.</td></tr>
<tr><td>PowerPC</td><td><code>b 0</code></td><td>Branch to itself</td></tr>
<tr><td>PowerPC</td><td><code>wait</code></td><td></td></tr>
<tr><td>x86</td><td><code>hlt</code></td><td></td></tr>
<tr><td>x86</td><td><code>mwait</code></td><td></td></tr>
</tbody></table>
<p>Hypersimulation should be as non-intrusive as possible, the only difference that should be noticeable as a Simics user is the increased performance. Registers, timing, memory contents, exceptions, interrupts etc. should be identical.</p>
<p>Hypersimulation using the hypersim-pattern-matcher may have some intrusions regarding Simics features:</p>
<ul>
<li>Device and memory access count will be too low if accesses are optimized away.</li>
<li>Breakpoints inside a hypersim detected code segment will not trigger every time.</li>
<li>Breakpoints on accesses to memory or devices will not hit every time since many of these accesses can have been optimized away.</li>
</ul>
<p>Hypersimulation using the hypersim-pattern-matcher is activated by default, and can be activated/deactivated with <code>enable-hypersim</code>/<code>disable-hypersim</code>.</p>
<p>The <code>hypersim-status</code> command gives some details on what hypersim features that are currently active.</p>
<p>Hypersim patterns are typically fragile, since they depend on an exact instruction pattern. Simply changing the compiler revision or an optimizing flag to the compiler can break the pattern from being recognized.</p>
<p>The <code>QSP-x86</code> machine does not use hypersim patterns, but with an old PPC-based machine we run the following example:</p>
<pre><code class="language-simics">simics&gt; <strong>disable-hypersim</strong>
simics&gt; <strong>system-perfmeter -realtime -mips</strong>
Using real time sample slice of 1.000000s
simics&gt; <strong>c</strong>
SystemPerf: Total-vt Total-rt Sample-vt Sample-rt Slowdown  CPU Idle  MIPS
SystemPerf: -------- -------- --------- --------- -------- ---- ---- -----
SystemPerf:     0.1s     0.3s     0.09s     0.33s      3.4 100%   0%    29
SystemPerf:     0.7s     1.3s     0.56s     1.00s      1.8  97%   0%    55
SystemPerf:     0.8s     2.3s     0.13s     1.00s      7.6  99%   0%    13
SystemPerf:     2.0s     3.3s     1.22s     1.00s      0.8  95%   0%   122
SystemPerf:     4.2s     4.3s     2.24s     1.00s      0.4  78%   0%   223
SystemPerf:     5.8s     5.3s     1.54s     1.00s      0.6  97%   0%   153
SystemPerf:    11.3s     6.3s     5.46s     1.00s      0.2  99%   0%   543
SystemPerf:    15.9s     7.3s     4.65s     1.00s      0.2  98%   0%   462
SystemPerf:    21.7s     8.3s     5.82s     1.00s      0.2  99%   0%   579
SystemPerf:    27.5s     9.3s     5.82s     1.00s      0.2 100%   0%   579
SystemPerf:    33.3s    10.3s     5.80s     1.00s      0.2  99%   0%   579

simics&gt; <strong>enable-hypersim</strong>
simics&gt; <strong>c</strong>
SystemPerf:    65.6s    11.2s    32.23s     0.88s      0.0  98%  85%  3673
SystemPerf:   491.1s    12.2s   425.52s     1.00s      0.0 100% 100% 42382
SystemPerf:   908.4s    13.2s   417.36s     1.00s      0.0  99% 100% 41550
SystemPerf:  1305.9s    14.2s   397.44s     1.00s      0.0 100% 100% 39745
SystemPerf:  1746.3s    15.2s   440.44s     1.00s      0.0  99% 100% 44039
SystemPerf:  2200.9s    16.2s   454.59s     1.00s      0.0  99% 100% 45457
</code></pre>
<p>This configuration has a Linux idle loop optimizer by default. We disable hypersim and execute the code "normally" during boot. After 6 seconds (host) or 12 seconds (virtual) the boot is finished and the operating system starts executing the idle loop. The idle loop itself is executed quickly in Simics, running at 579 MIPS. When idling, almost 6 virtual seconds is executed for each host second. That is, Simics executes 6 times faster than the hardware (the processor is configured to be running at 100 MHz).</p>
<p>Next, we stop the execution, enable hypersim, and continue the simulation. Now we can see the idle loop optimizer kicking in and 400 virtual seconds is executed each host second, that is about 70 times faster than without hypersim enabled.</p>
<h2 id="vmp"><a href="#vmp">4.1.6 VMP</a></h2>
<p>The VMP add-on for Simics makes use of hardware virtualization support to provide vastly improved performance when simulating x86-based systems. It is an optional part of the x86-based models.</p>
<p>The VMP feature requires that the host machine running Simics has the Intel® Virtualization Technology (Intel® VT) for IA-32, Intel® 64 and Intel® Architecture (Intel® VT-x) enabled in the host machine firmware (the BIOS).</p>
<p>Almost all Intel® processors since the original Core™ architecture, including Xeon® server processors, support the Intel® VT-x feature set. See the Intel® ARK at ark.intel.com for information on specific products.</p>
<p>The Intel® VT-x virtualization feature and the NX execute protection feature must also be enabled in the host machine firmware to run VMP. Look for options under either <em>Security</em> or <em>Virtualization</em> to find where to enable Intel® VT and NX in your firmware. If Intel® Trusted Execution Technology (TXT) is enabled in the firmware, it can also happen that the Intel® VT-x virtualization feature is restricted to trusted applications only; there is usually a special firmware setting controlling this behavior. VMP requires Intel® VT-x to be generally available.</p>
<p>All major features of Simics, including full inspectability of simulated state, and ability to model heterogeneous systems, are fully supported when running with VMP.</p>
<h3 id="installing-the-vmp-kernel-module"><a href="#installing-the-vmp-kernel-module">4.1.6.1 Installing the VMP Kernel Module</a></h3>
<h4 id="linux"><a href="#linux">4.1.6.1.1 Linux</a></h4>
<p>Installing and managing VMP kernel modules requires sudo privileges. Installing will compile the kernel module and therefore also requires an environment to build kernel modules. Which packages you need for building kernel modules depend on the distribution of Linux that you are using, but at least for certain Red Hat based distributions you would need <code>gcc-c++</code>, <code>kernel-headers</code>, and <code>kernel-devel</code>. Change directory to the user project and run:</p>
<pre><code>[project]$ bin/vmp-kernel-install
</code></pre>
<p>The script will build and then load the VMP kernel module. It will not install it permanently however, but it will tell you how to do that.</p>
<p>Disable VMP temporarily by running the <code>bin/vmp-kernel-unload</code> script, and enable VMP with the <code>bin/vmp-kernel-load</code> script. Permanently uninstall VMP from your host by running the <code>bin/vmp-kernel-uninstall</code> script.</p>
<p>If the installation is read-only, or if you for some other reason want to have the built VMP artifacts outside of the installation, you can give a directory to the relevant VMP scripts, for example:</p>
<pre><code>[project]$ bin/vmp-kernel-install /somewhere/directory
</code></pre>
<h4 id="windows"><a href="#windows">4.1.6.1.2 Windows</a></h4>
<p>The kernel module can be loaded and unloaded by running the <code>bin\vmp-kernel-load.bat</code> respective <code>bin\vmp-kernel-unload.bat</code> scripts as administrator. To do that, open a command shell as administrator and run:</p>
<pre><code>[project]&gt; bin\vmp-kernel-load.bat
</code></pre>
<p>Another way to perform the same action would be to right-click on <code>vmp-kernel-load.bat</code> and select run as administrator.</p>
<p>The <code>/AUTO</code> and <code>/DEMAND</code> options select the start option for the VMP service. With <code>/AUTO</code> (default), the service will be available after restart whereas with <code>/DEMAND</code> makes the service available just until shutdown or reboot, and then VMP has to be loaded again when needed.</p>
<p>If the script fails, see the Windows event log for more information. The most common reason is that Intel® VT-x technology or the NX feature is not enabled in the UEFI/BIOS. The kernel module will also fail to load if the Hyper-V feature is enabled.</p>
<h4 id="hyper-v-interaction"><a href="#hyper-v-interaction">4.1.6.1.3 Hyper-V Interaction</a></h4>
<p>Windows 8 and later can be run with Hyper-V enabled. Hyper-V is a feature where the operating system is run as a guest under the control of a hypervisor. VMP cannot be used in conjunction with Hyper-V: loading the VMP kernel module will fail.</p>
<p>There are various ways to disable the Hyper-V feature. Below are instructions for adding an additional boot configuration database entry to make it easy to boot with Hyper-V disabled:</p>
<ul>
<li>
<p>Open cmd prompt as <strong>administrator</strong>. In the admin prompt, run the following command to duplicate the current boot configuration entry:</p>
<pre><code>bcdedit /copy {current} /d "Win 8.1 with Hyper-V disabled"
</code></pre>
<p>This will print the id of the newly created entry, e.g.</p>
<pre><code>The entry was successfully copied to {41f7b750-485b-4f02-9d0e-4c8e3d02c31d}.
</code></pre>
</li>
<li>
<p>Run the following command (replace the id below with the id just printed):</p>
<pre><code>bcdedit /set {41f7b750-485b-4f02-9d0e-4c8e3d02c31d} hypervisorlaunchtype off
</code></pre>
</li>
<li>
<p>Restart the computer with boot option selection (click on Restart while holding down the shift key). Windows will present a blue screen where you can select the newly created boot entry.</p>
</li>
</ul>
<h3 id="running-with-the-vmp-add-on"><a href="#running-with-the-vmp-add-on">4.1.6.2 Running with the VMP Add-on</a></h3>
<p>With the VMP kernel modules installed, VMP will be enabled by default for each processor. You can disable VMP by running the <code>disable-vmp</code> command.</p>
<p>Due to details about how the Intel® VT feature that VMP is based on works, the acceleration may not kick in. Use the system-perfmeter to find out if a processor actually uses the VMP execution mode. To find out why VMP is not used, either raise the log level of the CPU in question or use the <code>info</code> command on the CPU.</p>
<h3 id="current-limitations-of-the-vmp-add-on"><a href="#current-limitations-of-the-vmp-add-on">4.1.6.3 Current Limitations of the VMP Add-on</a></h3>
<p>The VMP packages are currently available and supported for Linux and Windows.</p>
<div class="note">
<p>Due to bugs in the performance counters on many Intel CPU types, precise instruction counting cannot be achieved with VMP. The only CPU types that we are aware of that do not have this bug is Intel® Core™ Duo/Solo, and Pentium® 4 processors. This results in that slight differences in timing between runs will appear and cannot be avoided due to the bug in the underlying hardware.</p>
</div>
<div class="note">
<p>Software using hardware virtualization frequently have problems co-existing on a machine. If you are running other software using hardware virtualization such as VMware, VirtualBox, KVM, or Intel® Hardware Accelerated Execution Manager (Intel® HAXM), you may need to unload the driver corresponding to that software before using VMP.</p>
</div>
<p>Note that virtualizers such as VMware or Xen in most versions do not expose the Intel® VT feature. Even virtualizers that do expose the Intel® VT feature are likely to either lack support for the performance counters needed to run with VMP, or will have very different performance characteristics compared to native solutions. To summarize, it is highly recommended that VMP is run in a non-virtualized environment.</p>
<h3 id="differences-between-vmp-and-non-vmp"><a href="#differences-between-vmp-and-non-vmp">4.1.6.4 Differences Between VMP and non-VMP</a></h3>
<p>VMP is designed to be a transparent performance enhancement. The implementation with Intel® VT-x makes that impossible in some areas, and in some other areas we have taken the chance to utilize the VMP mode to tweak performance further. Enabling features that cannot be emulated using Intel® VT-x will automatically disable VMP.</p>
<p>VMP is optimized for software development, and the default timing parameters used in VMP reflect that. The timing settings can be viewed with the <code>info</code> command on the CPU object. As default, VMP timing will stall the PAUSE instruction, the RDTSC instruction, and each port-mapped I/O access for 10 milliseconds. These stalls are added to allow loops containing these operations to consume more virtual time per iteration and therefore speed up simulation if the loop is terminated after a set virtual time. This is controlled by attributes in the CPU object and can be changed if somewhat more realistic timing is more important than maximum simulation speed.</p>
<p>The hardware performance counters are used to figure out the number of executed instructions when running in VMP mode. Since those counters count string instructions as one regardless of the number of iterations, this is the instruction count model that must be used to enable VMP. This is controlled by the one_step_per_string_instruction attribute, and setting that to classic counting will disable VMP.</p>
<p>The TLB model is bypassed in VMP mode. To simulate a particular configuration of the TLB, or listen to the haps that the TLB model generates, VMP has to be manually disabled.</p>
<h2 id="performance-tweaks"><a href="#performance-tweaks">4.1.7 Performance Tweaks</a></h2>
<p>There are a number of parameters in Simics which can be tweaked which might lead to increased performance.</p>
<ul>
<li><em>Multicore Accelerator</em> can be enabled.</li>
<li>The instruction per cycle (IPC) parameter can be decreased, see command <code>&lt;cpu&gt;.set-step-rate</code>. Similar to reducing the CPU frequency, this will cause virtual time to progress more rapidly with the same amount of instructions executed.</li>
<li>Devices sometimes have a timing model that can be changed by attributes.</li>
<li>When real-time performance is required, the <code>real-time</code> module provides some means to achieve this. See command <code>enable-real-time-mode</code>.</li>
<li>For distributed simulation, multithreading, and Multicore Accelerator, you might want to tweak the default latency or allowed virtual time window. Increasing the latency/time window will diminish the cost of synchronizing the simulation. More information is available in chapter <a class="reference" href="scaling-simics.html">4.2</a>.</li>
<li>In a similar way, you may want to check the time quantum used to schedule the simulated processors within a Simics process or a simulation cell. Processors unrelated to each other (they do not share memory, for example) do not need to be simulated with a small time quantum.</li>
<li>The frequencies of the processors can be lowered (<code>cpu_freq_mhz</code>). This will cause virtual time to progress more rapidly with the same amount of instructions executed.</li>
<li>If page/swap activities can be monitored on the host running Simics you might want to decrease the memory-limit. See the <code>set-image-memory-limit</code> command. See chapter <a class="reference" href="managedisks.html#reducing-memory-usage-due-to-images">2.5.2.3</a>.</li>
<li>For large configuration using many cells and/or processors, Simics will create as many execution threads as there are available host processors and schedule the execution on these. However, sometimes it is not beneficial to use the SMT threads on the host but instead reduce the amount of created threads, using only 1 SMT thread per core. In that case, use the <code>set-thread-limit</code> command to limit the thread count to the available number of host cores.</li>
<li>The <code>sim-&gt;max_worker_threads</code> attribute controls the number of threads that can be used for JIT compilation. The default number of JIT threads is automatically set conservatively low, to avoid interfering with the execution threads. This attribute can be adjusted at any time. To disable parallel JIT compilation, the <code>sim-&gt;use_jit_threads</code> attribute can be set to <code>false</code>. Then the execution threads themselves run the compilation.</li>
</ul>

<div class="chain">
<a href="performance.html">4 Performance</a>
<a href="scaling-simics.html">4.2 Scaling Simics</a>
</div>